{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9215e205",
   "metadata": {},
   "source": [
    "# PS6: Drug Combination Design using Reinforcement Learning\n",
    "In this problem set, we apply Q-learning to design drug combinations that maximize therapeutic effectiveness while satisfying budget constraints. The goal is to learn an optimal policy that selects drug types and dosage levels through repeated interaction with an environment.\n",
    "\n",
    "> __Learning Objectives:__\n",
    "> \n",
    "> By the end of this lab, you will be able to:\n",
    "> * **Implement Q-learning for constrained optimization**: Build a Q-learning agent that learns state-action value functions through temporal difference updates, demonstrating how reinforcement learning discovers optimal policies without explicit knowledge of environment dynamics.\n",
    "> * **Model combinatorial drug design problems**: Encode drug cocktail design as a Markov decision process with discrete state spaces (dosage levels) and action spaces (drug selection), and apply Cobb-Douglas utility functions to evaluate therapeutic effectiveness under budget constraints.\n",
    "> * **Extract policies from learned Q-values**: Derive optimal drug selection and dosage policies by analyzing converged Q-value tables, mapping each state (current dosage configuration) to the action (drug combination) that maximizes expected cumulative reward.\n",
    "\n",
    "In this problem set, you will implement the __experiment__ function that our Q-learning agent will interact with to learn an optimal drug combination policy.\n",
    "\n",
    "Let's get started!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913f2c7e",
   "metadata": {},
   "source": [
    "## Problem\n",
    "The Drug Cocktail Design Problem involves selecting a combination of drugs to create a cocktail that maximizes therapeutic effectiveness while staying within budget and safety constraints. Each drug has attributes such as dosage, mechanism of action, and side effects, which contribute to the overall effectiveness of the cocktail. The drug cocktail design problem can be formulated as the following optimization problem:\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{align*}\n",
    "\\text{maximize} \\quad & U(n_1, \\dots, n_K) = \\kappa(\\gamma) \\prod_{i=1}^{K} n_i^{\\gamma_i} \\\\\n",
    "\\text{subject to} \\quad & \\sum_{i=1}^{K} c_i (n_i\\;W) \\leq B\\quad\\text{(budget constraint)}\\\\\n",
    "& \\sum_{i=1}^{K} n_i \\leq S\\quad\\text{(safety constraint)}\\\\\n",
    "& n_{i}^{\\text{min}} \\leq n_i \\leq n_i^{\\text{max}} \\quad \\forall i \\in \\{1, \\dots, K\\}\n",
    "\\end{align*}}\n",
    "$$\n",
    "where the objective function is a Cobb-Douglas utility function representing the effectiveness of the drug cocktail, and the constraints ensure that the cocktail stays within budget and safety limits. The design variables are the concentrations of each drug in the cocktail, denoted by $n_i$ for drug $i$ (units: mg/kg), $K$ is the total number of available drugs, and $W$ is the patient weight (units: kg). The $\\kappa(\\gamma)$ term is defined as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\kappa(\\gamma) & = \\begin{cases}\n",
    "-1 & \\text{if any } \\gamma_i < 0 \\\\\n",
    "1 & \\text{if all } \\gamma_i \\geq 0\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "where coefficients $\\gamma_i$ represent the efficacy of each drug type, which must be learned from historical data or set based on expert knowledge.\n",
    "\n",
    "The first set of constraints ensures that the total cost of the drug cocktail does not exceed a specified budget $B$, where $c_i$ is the cost per unit concentration (e.g., USD/mg) of drug $i$. The second set of constraints ensures that the total concentration of all drugs in the cocktail does not exceed a safety limit $S$. Finally, each drug concentration is bounded by minimum and maximum allowable levels, denoted by $n_i^{\\text{min}}$ and $n_i^{\\text{max}}$, respectively.\n",
    "\n",
    "In this lab, we'll implement the budget constraint only.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c407178e",
   "metadata": {},
   "source": [
    "## Q-Learning Algorithm\n",
    "Q-learning iteratively estimates the state action-value function $Q(s, a)$ by conducting repeated experiments $t=1,2,\\ldots$ in the world $\\mathcal{W}$. \n",
    "In each experiment, an agent in state $s\\in\\mathcal{S}$ takes action $a\\in\\mathcal{A}$, receives a reward $r$, and (potentially) transitions to a new state $s^{\\prime}$. After each experiment $t$, the agent updates its estimate of $Q(s, a)$ using the update rule:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "Q_{t+1}(s,a)\\leftarrow{\\underbrace{Q_{t}(s,a)}_{\\text{old value}}}+\\alpha_{t}\\cdot\\underbrace{\\left(r+\\gamma\\cdot\\max_{a^{\\prime}\\in\\mathcal{A}}Q_{t}(s^{\\prime},a^{\\prime}) - Q_{t}(s,a)\\right)}_{\\text{new value}}\\quad{t = 1,2,3,\\ldots}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $0<\\alpha_{t} <{1}$ is the learning rate parameter at time $t$, and $0<\\gamma<{1}$ is the discount factor. \n",
    "We estimate the policy function $\\pi:\\mathcal{S}\\rightarrow\\mathcal{A}$ by selecting the action $a$ that maximizes $Q(s,a)$ at each state $s$:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\pi(s) = \\arg\\max_{a\\in\\mathcal{A}}Q(s,a)\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "### Algorithm\n",
    "Initialize $Q(s,a)$ arbitrarily for all $s\\in\\mathcal{S}$, and $a\\in\\mathcal{A}$.\n",
    "Set the hyperparameters: learning rate $\\alpha_{t}$, the discount factor $\\gamma$, the exploration rate $\\epsilon_{t}$, the maximum number of iterations $\\texttt{maxiter}$, and the convergence tolerance $\\delta$. Set $\\texttt{converged}\\gets\\texttt{false}$. \n",
    "\n",
    "For $s\\in\\mathcal{S}$\n",
    "1. Initialize the trial counter $t\\gets{1}$\n",
    "2. While $\\texttt{converged} $ is $\\texttt{false}$ __do__:\n",
    "    1. Roll a random number $p\\in[0,1]$. Compute $\\epsilon_{t}={t^{-1/3}}\\cdot\\left(K\\cdot\\log(t)\\right)^{1/3}$ where $K=|\\mathcal{A}|$ is the number of actions.\n",
    "    2. If $p\\leq\\epsilon_{t}$, choose a random (uniform) action $a_{t}\\in\\mathcal{A}$. Otherwise, choose a greedy action $a_{t} = \\text{arg}\\max_{a\\in\\mathcal{A}}{Q_{t}(s,a)}$.\n",
    "    3. Take action $a_{t}$, observe the reward $r$ from the __world__ and transition to the next state $s^{\\prime}$.\n",
    "    4. Update the state-action-value function: $Q_{t+1}(s,a)\\leftarrow{Q_{t}(s,a)}+\\alpha_{t}\\cdot\\underbrace{\\left(r+\\gamma\\cdot\\overbrace{\\max_{a^{\\prime}\\in\\mathcal{A}}Q_{t}(s^{\\prime},a^{\\prime})}^{\\text{one-step lookahead}} - Q_{t}(s,a)\\right)}_{\\text{new information}}$.\n",
    "    5. Update the state $s\\leftarrow{s^{\\prime}}$, the learning rate $\\alpha_{t+1}\\leftarrow\\alpha_{t}$, and the counter $t\\leftarrow{t+1}$\n",
    "    6. Convergence check: If the Q-table has bounded change $\\lVert{Q_{t+1} - Q_{t}}\\rVert\\leq\\delta$, then the algorithm has converged. Set $\\texttt{converged}\\gets\\texttt{true}$.\n",
    "    7. Otherwise: if $t\\geq\\texttt{maxiter}$, then set $\\texttt{converged}\\gets\\texttt{true}$ and notify the caller that the maximum iteration limit was reached without convergence. Proceed to next state.\n",
    "    8. Otherwise: continue to the next iteration.\n",
    "3. End While\n",
    "4. End For\n",
    "\n",
    "### Convergence\n",
    "Q-learning converges to the optimal policy under two key theoretical conditions (assuming the Markov property holds for the environment):\n",
    "* __Learning rate decay__: The learning rate $\\alpha_{t}$ must satisfy $\\sum_{t=0}^\\infty \\alpha_t(s, a) = \\infty$ and $\\sum_{t=0}^\\infty \\alpha_t^2(s, a) < \\infty$ for all state-action pairs, ensuring sufficient initial updates while stabilizing over time. Setting $\\alpha_{t+1} \\gets \\beta\\alpha_{t}$ where $\\beta<1$ is a common choice.\n",
    "* __Infinite exploration__: All state-action pairs must be visited infinitely often. This condition holds for $\\epsilon$-greedy policies with persistent exploration, i.e., $\\epsilon_{t} > 0\\,\\,\\forall{t}$.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5172ddc3",
   "metadata": {},
   "source": [
    "## Setup, Data, and Prerequisites\n",
    "First, we set up the computational environment by including the `Include.jl` file and loading any needed resources.\n",
    "\n",
    "> The [`include(...)` command](https://docs.julialang.org/en/v1/base/base/#include) evaluates the contents of the input source file, `Include.jl`, in the notebook's global scope. The `Include.jl` file sets paths, loads required external packages, etc. For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/). \n",
    "\n",
    "Let's set up our code environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "4cdd3b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"Include-solution.jl\")); # include the Include.jl file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acf7cfd",
   "metadata": {},
   "source": [
    "In addition to standard Julia libraries, we'll also use [the `VLDataScienceMachineLearningPackage.jl` package](https://github.com/varnerlab/VLDataScienceMachineLearningPackage.jl). Check out [the documentation](https://varnerlab.github.io/VLDataScienceMachineLearningPackage.jl/dev/) for more information on the functions, types, and data used in this material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e8ea5",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Let's implement the `experiment(...)` function that is used by our reinforcement learning agent to evaluate drug cocktail designs. This function will compute the utility of a given drug cocktail based on the Cobb-Douglas utility function and apply the necessary constraints.\n",
    "\n",
    "> __What is going on in the `experiment(...)` function?__\n",
    "> \n",
    "> This function takes in a context object, which contains information about the drug types, their costs, and levels. It also takes in two integers, `s` and `a`, which represent the state and action, respectively. The function computes the utility of the drug cocktail based on the Cobb-Douglas utility function and checks if the cocktail meets the budget and safety constraints. If the constraints are met, it returns the computed utility; otherwise, it returns a penalty value.\n",
    "\n",
    "We need to make a few changes from the lab template to implement the bounds and safety constraints.\n",
    "\n",
    "> __What changes are needed?__ \n",
    ">\n",
    "> In all cases, we will implement the constraints using squared violations to penalize constraint violations more heavily. You'll need to compute the budget, safety, and bounds violations as squared terms and include them in the final utility calculation with their respective penalties. Remember: we want to maximize utility, so penalties should be subtracted from the utility value.\n",
    "\n",
    "Implement the revised `experiment(...)` function in the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "03be1ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "experiment (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function experiment(context::MyExperimentalDrugCocktailContext, s::Int64, a::Int64)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Initialize from context\n",
    "    # -------------------------------\n",
    "    K = context.K\n",
    "    m = context.m\n",
    "    levels = context.levels\n",
    "    costs = context.cost\n",
    "    bounds = context.bounds\n",
    "    W = context.W\n",
    "    S = context.S\n",
    "    Î³ = context.Î³\n",
    "    B = context.B\n",
    "\n",
    "    N = 2^K\n",
    "    M = m^K\n",
    "\n",
    "    constraint_penalty = -10000.0\n",
    "    safety_penalty = -10000.0\n",
    "    bounds_penalty = -10000.0\n",
    "\n",
    "    # -------------------------------\n",
    "    # State and action representations\n",
    "    # -------------------------------\n",
    "    sáµ¢ = digits(s, base = m, pad = K)\n",
    "    aáµ¢ = digits(a, base = 2, pad = K)\n",
    "    if a == N\n",
    "        aáµ¢ = digits(a - 1, base = 2, pad = K)\n",
    "    end\n",
    "\n",
    "    # -------------------------------\n",
    "    # Active drugs\n",
    "    # -------------------------------\n",
    "    Sâ‚Š = findall(x -> x == 1, aáµ¢)\n",
    "    n = zeros(Float64, K)\n",
    "    tmp = 1.0\n",
    "\n",
    "    for i âˆˆ Sâ‚Š\n",
    "        level_index = sáµ¢[i]\n",
    "\n",
    "        if level_index == 0\n",
    "            n[i] = levels[i].high\n",
    "        elseif level_index == 1\n",
    "            n[i] = levels[i].nominal\n",
    "        elseif level_index == 2\n",
    "            n[i] = levels[i].low\n",
    "        end\n",
    "\n",
    "        tmp *= n[i]^Î³[i]\n",
    "    end\n",
    "\n",
    "    # -------------------------------\n",
    "    # Utility sign correction\n",
    "    # -------------------------------\n",
    "    Îº = any(Î³[Sâ‚Š] .< 0) ? -1.0 : 1.0\n",
    "\n",
    "    # -------------------------------\n",
    "    # Budget constraint\n",
    "    # -------------------------------\n",
    "    spent = sum(costs[i] * n[i] * W for i âˆˆ Sâ‚Š)\n",
    "    budget_violation = max(0.0, spent - B)^2\n",
    "\n",
    "    # -------------------------------\n",
    "    # Safety constraint\n",
    "    # -------------------------------\n",
    "    total_dosage = sum(n)\n",
    "    safety_violation = max(0.0, total_dosage - S)^2\n",
    "\n",
    "    # -------------------------------\n",
    "    # Bounds constraint\n",
    "    # -------------------------------\n",
    "    vâ‚‹ = zeros(Float64, K)\n",
    "    vâ‚Š = zeros(Float64, K)\n",
    "\n",
    "    for i âˆˆ Sâ‚Š\n",
    "        L = bounds[i, 1]\n",
    "        U = bounds[i, 2]\n",
    "        vâ‚‹[i] = max(0.0, L - n[i])^2\n",
    "        vâ‚Š[i] = max(0.0, n[i] - U)^2\n",
    "    end\n",
    "\n",
    "    total_bounds_violation = sum(vâ‚‹) + sum(vâ‚Š)\n",
    "\n",
    "    # -------------------------------\n",
    "    # Next state\n",
    "    # -------------------------------\n",
    "    sâ€² = s + 1\n",
    "    if sâ€² > M\n",
    "        sâ€² = 1\n",
    "    end\n",
    "\n",
    "    # -------------------------------\n",
    "    # Reward\n",
    "    # -------------------------------\n",
    "    U = Îº * tmp +\n",
    "        constraint_penalty * budget_violation +\n",
    "        safety_penalty * safety_violation +\n",
    "        bounds_penalty * total_bounds_violation\n",
    "\n",
    "    return (sâ€², U)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d58131",
   "metadata": {},
   "source": [
    "### Constants\n",
    "In this section, let's define some constants that will be used in our drug cocktail design problem. These constants include the number of drug types, their costs, and the levels of drug concentrations. See the comment next to each constant for its units, permitted values, and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "37f641d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3; # number of drug types\n",
    "m = 3;  # number of drug levels per type {high, nominal, low}\n",
    "ğ’œ = range(1, stop=2^K, step=1) |> collect; # action space: 2^K possible drug combinations (binary selection vector)\n",
    "ğ’® = range(1, stop=m^K, step=1) |> collect; # state space: m^K possible dosage level configurations\n",
    "W = 80.0; # weight of the patient in kg\n",
    "B = 1000.0; # TODO: you can change the budget in USD\n",
    "S = 200; # TODO: need to set a safety constraint for maximum allowable dosage units: mg/kg-day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688250f6",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf1b724",
   "metadata": {},
   "source": [
    "## Task 1: Setup the context model\n",
    "In this task, we will set up the context model for our drug cocktail design problem. This involves defining a mutable struct that holds the necessary information about the drugs, their costs, and levels. \n",
    "\n",
    "> __Context model explanation:__ The context model stores the problem structure including the number of drug types `K`, the number of dosage levels `m` per drug, the drug effectiveness coefficients `Î³` (which the agent does not know a priori), the unit costs per drug, the three dosage concentration levels (high, nominal, low) for each drug in mg/kg, the patient weight `W` in kg, and the total budget constraint `B` in USD. This encapsulates all the problem parameters that define the drug cocktail design environment.\n",
    "\n",
    "We need to make a few changes from the lab template to implement the bounds and safety constraints.\n",
    "\n",
    "> __What changes are needed?__\n",
    "> \n",
    "> We need to add the safety limit `S` and the minimum and maximum dosage levels for each drug type to the context model. This will allow us to enforce the safety and bounds constraints during the experiment evaluation. Note: we'll need to update the `MyExperimentalDrugCocktailContext` struct to include these new fields and the factory function that creates instances of this struct.\n",
    ">\n",
    "> Generate some dummy values for these new fields to test your implementation. For example, does your implementation respond correctly when the safety limit is exceeded or when drug dosages are outside their specified bounds?\n",
    "\n",
    "We save the context information in the `contextmodel::MyExperimentalDrugCocktailContext` variable below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "e4ffe604",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextmodel = let \n",
    "\n",
    "    # initialize - \n",
    "    costs = Dict{Int64, Float64}(); # cost per unit concentration (e.g., USD/mg) of drug i\n",
    "    levels = Dict{Int64, NamedTuple}(); # levels of drug concentrations for drug i\n",
    "    Î³ = Array{Float64}(undef, K); # effectiveness coefficients for each drug type\n",
    "\n",
    "    # generate random cost data -\n",
    "    for i âˆˆ 1:K\n",
    "        costs[i] = rand(0.1:0.1:1.0); # random cost between 0.1 and 1.0 USD/mg\n",
    "    end\n",
    "\n",
    "    # generate random level data -\n",
    "    for i âˆˆ 1:K\n",
    "        high = rand(5.0:1.0:10.0);    # high concentration level in mg/kg\n",
    "        nominal = rand(2.0:0.1:4.9);  # nominal concentration level in mg/kg\n",
    "        low = rand(1.0:0.1:1.9);      # low concentration level in mg/kg\n",
    "        levels[i] = (high=high, nominal=nominal, low=low);\n",
    "    end\n",
    "\n",
    "    # generate effectiveness coefficients -\n",
    "    for i âˆˆ 1:K\n",
    "        Î³[i] = randn(); # random normal efficacy value (negative: inhibitory, positive: excitatory)\n",
    "    end\n",
    "\n",
    "    # Drug bounds: generate bounds for each drug type -\n",
    "    Ïƒ = 0.25; # standard deviation for randomness\n",
    "    bounds = zeros(Float64, K, 2); # initialize bounds array\n",
    "    for i âˆˆ 1:K\n",
    "        bounds[i,1] = 0.0; # lower bound\n",
    "        bounds[i,2] = (1+Ïƒ*randn())*S; # upper bound based on safety constraint\n",
    "    end\n",
    "\n",
    "    # build the model -\n",
    "    model = build(MyExperimentalDrugCocktailContext, (\n",
    "\n",
    "        K = K, # number of drug types\n",
    "        m = m, # number of drug levels per type\n",
    "        Î³ = Î³, # effectiveness coefficients for each drug type (we don't know these a priori)\n",
    "        B = B, # total budget in USD\n",
    "        cost = costs, # cost per unit concentration (e.g., USD/mg) of drug i\n",
    "        levels = levels, # levels of drug concentrations for drug i\n",
    "        W = W, # weight of the patient in kg\n",
    "        S = S, # safety constraint for maximum allowable dosage units: mg/kg-day\n",
    "        bounds = bounds # bounds for each drug type (L,U)\n",
    "    ));\n",
    "    \n",
    "    model; # return the model\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fb9536",
   "metadata": {},
   "source": [
    "Let's examine the randomly generated problem parameters for our drug cocktail design instance. This table displays the unit costs, dosage concentration levels, and effectiveness coefficients for each drug type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "42c97063",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    # initialize -\n",
    "    df = DataFrame();\n",
    "    \n",
    "    # extract data from context model -\n",
    "    for i âˆˆ 1:K\n",
    "        row_df = (\n",
    "            drug = i,\n",
    "            unit_cost = contextmodel.cost[i],\n",
    "            low_conc = contextmodel.levels[i].low,\n",
    "            nominal_conc = contextmodel.levels[i].nominal,\n",
    "            high_conc = contextmodel.levels[i].high,\n",
    "            Î³ = contextmodel.Î³[i]\n",
    "        );\n",
    "        push!(df, row_df);\n",
    "    end\n",
    "    \n",
    "    # display the table -\n",
    "    pretty_table(\n",
    "        df;\n",
    "        backend = :text,\n",
    "        table_format = TextTableFormat(borders = text_table_borders__compact)\n",
    "    );\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4198fbc",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d632eb0",
   "metadata": {},
   "source": [
    "## Task 2: Construct the Learning Agent Model\n",
    "In this task, we will construct the learning agent model that evaluates different drug cocktail designs. This agent will run experiments using the `experiment(...)` function we implemented earlier and will learn to optimize the drug cocktail design over time to maximize utility while adhering to the constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "383fe3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyQLearningAgentModel([1, 2, 3, 4, 5, 6, 7, 8, 9, 10  â€¦  18, 19, 20, 21, 22, 23, 24, 25, 26, 27], [1, 2, 3, 4, 5, 6, 7, 8], 0.95, 0.1, [0.0 0.0 â€¦ 0.0 0.0; 0.0 0.0 â€¦ 0.0 0.0; â€¦ ; 0.0 0.0 â€¦ 0.0 0.0; 0.0 0.0 â€¦ 0.0 0.0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mylearningagent = let\n",
    "\n",
    "    # initialize -\n",
    "    Î³ = 0.95; # discount factor (for future rewards)\n",
    "    Î± = 0.1;  # learning rate\n",
    "    Q = Array{Float64}(undef, length(ğ’®), length(ğ’œ)); # Q-value table\n",
    "\n",
    "    # fill the Q-table with zeros -\n",
    "    fill!(Q, 0.0); # fast way to fill an array with a value\n",
    "\n",
    "    # build the learning agent model -\n",
    "    model = build(MyQLearningAgentModel, (\n",
    "        Î³ = Î³, # discount factor\n",
    "        Î± = Î±, # learning rate\n",
    "        Q = Q, # Q-value table\n",
    "        states = ğ’®, # state space\n",
    "        actions = ğ’œ # action space\n",
    "    ));\n",
    "\n",
    "    model; # return the model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825c4f81",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98740671",
   "metadata": {},
   "source": [
    "## Task 3: Let's let the agent learn!\n",
    "In this task, we will allow the learning agent to interact with the environment and learn from its experiences. The agent will use the `experiment(...)` function to evaluate different drug cocktail designs and update its policy based on the observed rewards. \n",
    "\n",
    "> __What is going on in this code cell?__ We call the `solve(...)` method with our Q-learning agent, the context model, and the `experiment(...)` function as the world model. The agent will run up to 10,000 iterations, exploring different state-action pairs using epsilon-greedy selection. For each iteration, it takes an action (selecting drugs and dosages), observes the reward from the `experiment(...)` function (utility minus budget violation penalties), and updates the Q-value table using the temporal difference learning rule. The algorithm terminates when the maximum change in Q-values falls below the convergence tolerance Î´ = 0.0001 or when the maximum number of iterations is reached.\n",
    "\n",
    "The `result` variable stores the trained Q-learning model containing the converged Q-value table `result.Q`, which encodes the expected cumulative reward for each state-action pair after learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "f29c5ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = solve(mylearningagent, contextmodel; \n",
    "    maxsteps = 10000, Î´ = 0.0001, worldmodel = experiment);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "13dc1ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27Ã—8 Matrix{Float64}:\n",
       " -0.524679  -0.04571    -0.030659   â€¦  -0.028067   -0.027985   -0.0289544\n",
       " -0.588611  -0.0473821  -0.0491033     -0.046947   -0.0463926  -0.0465956\n",
       " -0.404642  -0.135545   -0.0828626     -0.071693   -0.0718353  -0.0719236\n",
       " -0.2241    -0.161041   -0.0917825     -0.0885948  -0.0856783  -0.0888565\n",
       " -0.450474  -0.111038   -0.151398      -0.107442   -0.107659   -0.107351\n",
       " -0.357419  -0.228581   -0.160997   â€¦  -0.12972    -0.131465   -0.12924\n",
       " -0.200317  -0.171571   -0.22788       -0.123927   -0.125585   -0.12442\n",
       " -0.41169   -0.181176   -0.271828      -0.104415   -0.104132   -0.10441\n",
       " -0.344375  -0.0507478  -0.0513708     -0.0506514  -0.0504251  -0.0512923\n",
       " -0.414389  -0.064398   -0.0744323     -0.0668519  -0.0642204  -0.0639165\n",
       "  â‹®                                 â‹±   â‹®                      \n",
       " -0.426085  -0.0805798  -0.0781632     -0.0758556  -0.0742406  -0.0738743\n",
       " -0.521594  -0.117276   -0.115104      -0.113986   -0.117297   -0.114878\n",
       " -0.363326  -0.22143    -0.189043   â€¦  -0.186708   -0.194643   -0.187333\n",
       " -0.594496  -0.226878   -0.220028      -0.220528   -0.21053    -0.210432\n",
       " -0.289993  -0.296972   -0.275519      -0.270865   -0.276779   -0.271397\n",
       " -0.616411  -0.35712    -0.368733      -0.355034   -0.355752   -0.375762\n",
       " -0.578129  -0.369899   -0.370438      -0.352938   -0.350584   -0.351147\n",
       " -0.397641  -0.259154   -0.258181   â€¦  -0.259382   -0.256533   -0.256535\n",
       " -0.408245  -0.0347587  -0.0306479     -0.0204791  -0.0208388  -0.0202834"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result.Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6d35c",
   "metadata": {},
   "source": [
    "Let's extract the policy from our trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "fa135fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ï€(s) = mypolicy(result.Q)[s]; # define a policy function that returns the best action for state s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ae003",
   "metadata": {},
   "source": [
    "What's in the policy? Let's pick the best action for each state according to the learned Q-values. We'll display these in a table.\n",
    "\n",
    "> __What's going on in this table?__ For a given state `s` (representing a specific dosage level configuration), we extract the optimal action `a` from the learned policy. We then decode the action into a binary vector to identify which drugs are selected, and decode the state to determine the dosage level (high, nominal, or low) for each selected drug. \n",
    "> \n",
    "> The table displays the drug index, dosage level, unit cost, concentration levels available, total spending for that drug (concentration Ã— patient weight Ã— unit cost), the effectiveness coefficient Î³, and the Q-value for this state-action pair. This shows the recommended drug cocktail composition that maximizes expected utility while respecting budget constraints for each state (dose combinations).\n",
    "\n",
    "The table reveals which drugs the agent recommends including in the cocktail, at which dosage levels, and the expected value of following this recommendation from the given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "253768a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "\n",
    "    # initialize -\n",
    "    df = DataFrame();\n",
    "    s = 15; # what state do we want to look at?\n",
    "    a = Ï€(s); # this gives me the best action *index* for state s\n",
    "    N = 2^K; # number of actions\n",
    "    M = m^K; # number of states\n",
    "    Î³ = contextmodel.Î³; # effectiveness coefficients for each drug type\n",
    "\n",
    "    aáµ¢ = digits(a, base=2, pad=K); # action vector representation\n",
    "    if (a == N)\n",
    "        aáµ¢ = digits(a-1, base=2, pad=K); # adjust for all-drug case\n",
    "    end\n",
    "    Sâ‚Š = findall(x -> x == 1, aáµ¢); # indices of drugs being administered\n",
    "\n",
    "    for i âˆˆ eachindex(Sâ‚Š)\n",
    "        drug_index = Sâ‚Š[i];\n",
    "        level_index = digits(s, base=m, pad=K)[drug_index];\n",
    "\n",
    "        level_str = \"\";\n",
    "        if (level_index == 0)\n",
    "            level_str = \"high\";\n",
    "        elseif (level_index == 1)\n",
    "            level_str = \"nominal\";\n",
    "        elseif (level_index == 2)\n",
    "            level_str = \"low\";\n",
    "        end\n",
    "        n = contextmodel.levels[drug_index];\n",
    "        spend = 0.0;\n",
    "        if level_index == 0\n",
    "            spend = (W*n.high)*contextmodel.cost[drug_index];\n",
    "        elseif level_index == 1\n",
    "            spend = (W*n.nominal)*contextmodel.cost[drug_index];\n",
    "        elseif level_index == 2\n",
    "            spend = (W*n.low)*contextmodel.cost[drug_index];\n",
    "        end\n",
    "\n",
    "        push!(df, (\n",
    "            state = s,\n",
    "            action = a,\n",
    "            drug = drug_index,\n",
    "            level = level_str,\n",
    "            unitcost = contextmodel.cost[drug_index],\n",
    "            concentration = n,\n",
    "            spend = spend,\n",
    "            Î³ = Î³[drug_index],\n",
    "            q_value = result.Q[s, a]\n",
    "        ));\n",
    "    end\n",
    "\n",
    "    # make a pretty table -\n",
    "    pretty_table(\n",
    "        df;\n",
    "        backend = :text,\n",
    "        fit_table_in_display_horizontally = false,\n",
    "        table_format = TextTableFormat(borders = text_table_borders__compact)\n",
    "    );\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407dbc4",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865bf20a",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "This problem uses randomly generated parameters, so we're interested in checking whether our implementation responds reasonably to edge cases. Let's explore a few scenarios that test the Q-learning agent's behavior under different constraint conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8756e01",
   "metadata": {},
   "source": [
    "### Test case 1: Small budget constraint\n",
    "\n",
    "> __Test:__ \n",
    "> What happens when the budget is so small that no drug combination is feasible? Set a small budget value, e.g., $B = 50$ or $B = 100$ USD, and retrain the agent with this new budget constraint. Examine the learned policy to see which drugs and dosages are selected. \n",
    "\n",
    "Check whether the policy selects cheaper drugs over expensive ones and whether the selected dosage levels are predominantly low rather than high. \n",
    "\n",
    "__Answer__: Describe what happened in test case 1 here ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "7e4c542d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "did_I_do_test_case_1 = true; # With a very small budget (e.g., B = 50 or 100), the agent learns to avoid expensive drug combinations, typically selecting either no drugs or only the cheapest drugs at low dosage levels to minimize budgetâ€violation penalties.\n",
    "# set to true if you did test case 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac31857d",
   "metadata": {},
   "source": [
    "### Test case 2: Negative efficacy coefficients\n",
    "\n",
    "> __Test:__ \n",
    "> What happens when the most cost-effective drug has negative efficacy? Set the efficacy coefficients $\\gamma$ such that at least one drug has a strongly negative value, e.g., $\\gamma = [1.5, -2.0, 0.8]$. Retrain the agent with these efficacy values and examine which drugs appear in the learned policy.\n",
    "\n",
    "Check whether the learned policy systematically excludes drugs with negative $\\gamma$ values.\n",
    "\n",
    "__Answer__: Describe what happened in test case 2 here ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "33df1dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "did_I_do_test_case_2 = true; # When a drug has a strongly negative efficacy (e.g., Î³ = âˆ’2.0), the learned policy consistently avoids selecting that drug, even if it is cheap, and instead favors combinations using only drugs with positive Î³ values to prevent negative utility.\n",
    "# set to true if you did test case 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6f6ef5",
   "metadata": {},
   "source": [
    "### Test case 3: Tight safety constraint\n",
    "\n",
    "> __Test:__ \n",
    "> What happens when even the minimum dosage of a single drug exceeds the safety limit? Set a low safety limit, e.g., $S = 10$ or $S = 5$ mg/kg-day, while keeping the budget generous. Retrain the agent with this tight safety constraint and analyze the resulting policy.\n",
    "\n",
    "Check whether the policy respects the safety constraint by keeping $\\sum_{i}n_{i} \\leq S$ even when the budget allows for expensive high-dose drugs.\n",
    "\n",
    "__Answer__: Describe what happened in test case 3 here .... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "2874ad95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "did_I_do_test_case_3 = true; # With a very tight safety limit (e.g., S = 5 or 10), the agent learns to avoid administering drugs altogether or selects only minimal/low dosages, since any higher dose incurs a large safety penalty, even when the budget is not binding.\n",
    "# set to true if you did test case 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abe512a",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e12ed",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this lab, we applied Q-learning to solve the drug cocktail design problem, learning optimal policies for selecting drug combinations and dosage levels under budget constraints.\n",
    "\n",
    "> __Key Takeaways:__\n",
    "> \n",
    "> * **Q-learning for combinatorial optimization**: We implemented a Q-learning agent that iteratively estimates state-action value functions through temporal difference updates, converging to an optimal policy without requiring explicit knowledge of transition probabilities or reward functions. The algorithm balanced exploration (trying new drug combinations) with exploitation (selecting known effective combinations) using an epsilon-greedy strategy with a decaying exploration rate.\n",
    "> * **MDP formulation for drug design**: We encoded the drug cocktail problem as a Markov decision process with states representing dosage level configurations (high, nominal, low for each drug type), actions representing binary drug selection vectors, and rewards computed from Cobb-Douglas utility functions penalized for budget violations. This formulation captured both therapeutic effectiveness (through preference parameters Î³áµ¢) and resource constraints (through budget limits).\n",
    "> * **Policy extraction and interpretation**: We derived the optimal policy by selecting argmax actions from the converged Q-value table, mapping each state to the drug combination that maximizes expected cumulative reward. The learned policy encoded both drug selection decisions (which drugs to include) and dosage optimization (which concentration levels to use), demonstrating how reinforcement learning discovers structured solutions to constrained combinatorial problems.\n",
    "\n",
    "Q-learning provides a framework for sequential decision-making in optimization problems where the relationship between actions and outcomes must be learned through experience rather than analytical derivation.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5bb268",
   "metadata": {},
   "source": [
    "## Tests\n",
    "The code block below shows how we implemented the tests and what we are testing. In these tests, we check values in your notebook and give feedback on which items are correct, missing, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "e9e0d10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"CHEME 5800 PS6 Test Suite\", Any[Test.DefaultTestSet(\"Experiment Function Tests\", Any[], 8, false, false, true, 1.765904650782395e9, 1.765904650782579e9, false, \"/Users/shreyaparmar/Downloads/ps6-cheme-5800-f25-sp2686/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X52sZmlsZQ==.jl\", Xoshiro(0x069b8ac5f04bc8aa, 0xb36b83b12799fc58, 0x8af7d0d8f59ce782, 0x60a6e2b1767b5af1, 0x7c455cf07089bfd3)), Test.DefaultTestSet(\"Context Model Tests\", Any[], 46, false, false, true, 1.765904650782597e9, 1.765904650782788e9, false, \"/Users/shreyaparmar/Downloads/ps6-cheme-5800-f25-sp2686/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X52sZmlsZQ==.jl\", Xoshiro(0x069b8ac5f04bc8aa, 0xb36b83b12799fc58, 0x8af7d0d8f59ce782, 0x60a6e2b1767b5af1, 0x7c455cf07089bfd3)), Test.DefaultTestSet(\"Learning Agent Tests\", Any[], 16, false, false, true, 1.765904650782798e9, 1.765904650782875e9, false, \"/Users/shreyaparmar/Downloads/ps6-cheme-5800-f25-sp2686/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X52sZmlsZQ==.jl\", Xoshiro(0x069b8ac5f04bc8aa, 0xb36b83b12799fc58, 0x8af7d0d8f59ce782, 0x60a6e2b1767b5af1, 0x7c455cf07089bfd3)), Test.DefaultTestSet(\"Q-Learning Results Tests\", Any[], 7, false, false, true, 1.765904650782881e9, 1.765904650782965e9, false, \"/Users/shreyaparmar/Downloads/ps6-cheme-5800-f25-sp2686/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X52sZmlsZQ==.jl\", Xoshiro(0x069b8ac5f04bc8aa, 0xb36b83b12799fc58, 0x8af7d0d8f59ce782, 0x60a6e2b1767b5af1, 0x7c455cf07089bfd3)), Test.DefaultTestSet(\"Policy Extraction Tests\", Any[], 15, false, false, true, 1.76590465078297e9, 1.765904650783061e9, false, \"/Users/shreyaparmar/Downloads/ps6-cheme-5800-f25-sp2686/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X52sZmlsZQ==.jl\", Xoshiro(0x069b8ac5f04bc8aa, 0xb36b83b12799fc58, 0x8af7d0d8f59ce782, 0x60a6e2b1767b5af1, 0x7c455cf07089bfd3)), Test.DefaultTestSet(\"Constants and Setup Tests\", Any[], 16, false, false, true, 1.765904650783067e9, 1.765904650783096e9, false, \"/Users/shreyaparmar/Downloads/ps6-cheme-5800-f25-sp2686/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X52sZmlsZQ==.jl\", Xoshiro(0x069b8ac5f04bc8aa, 0xb36b83b12799fc58, 0x8af7d0d8f59ce782, 0x60a6e2b1767b5af1, 0x7c455cf07089bfd3)), Test.DefaultTestSet(\"Integration Tests\", Any[], 9, false, false, true, 1.765904650783102e9, 1.765904650783202e9, false, \"/Users/shreyaparmar/Downloads/ps6-cheme-5800-f25-sp2686/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X52sZmlsZQ==.jl\", Xoshiro(0x069b8ac5f04bc8aa, 0xb36b83b12799fc58, 0x8af7d0d8f59ce782, 0x60a6e2b1767b5af1, 0x7c455cf07089bfd3)), Test.DefaultTestSet(\"Completion Confirmation Tests\", Any[], 3, false, false, true, 1.765904650783207e9, 1.765904650783215e9, false, \"/Users/shreyaparmar/Downloads/ps6-cheme-5800-f25-sp2686/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X52sZmlsZQ==.jl\", Xoshiro(0x069b8ac5f04bc8aa, 0xb36b83b12799fc58, 0x8af7d0d8f59ce782, 0x60a6e2b1767b5af1, 0x7c455cf07089bfd3))], 0, false, true, true, 1.765904650782342e9, 1.765904650783215e9, false, \"/Users/shreyaparmar/Downloads/ps6-cheme-5800-f25-sp2686/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X52sZmlsZQ==.jl\", Xoshiro(0x069b8ac5f04bc8aa, 0xb36b83b12799fc58, 0x8af7d0d8f59ce782, 0x60a6e2b1767b5af1, 0x7c455cf07089bfd3))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@testset verbose = true \"CHEME 5800 PS6 Test Suite\" begin\n",
    "\n",
    "    @testset \"Experiment Function Tests\" begin\n",
    "        # Test 1: Check if experiment function exists and is callable\n",
    "        @test isdefined(Main, :experiment) == true\n",
    "        \n",
    "        # Test 2: Check experiment function signature (should accept context, state, action)\n",
    "        @test hasmethod(experiment, (MyExperimentalDrugCocktailContext, Int64, Int64)) == true\n",
    "        \n",
    "        # Test 3: Test experiment returns a tuple (s', reward)\n",
    "        test_result = experiment(contextmodel, 1, 1)\n",
    "        @test isa(test_result, Tuple) == true\n",
    "        @test length(test_result) == 2\n",
    "        \n",
    "        # Test 4: Check that new state is within valid range\n",
    "        mid_state = min(5, m^K)  # Use a state that exists regardless of dimension\n",
    "        mid_action = min(3, 2^K)  # Use an action that exists regardless of dimension\n",
    "        s_prime, reward = experiment(contextmodel, mid_state, mid_action)\n",
    "        @test s_prime >= 1 && s_prime <= m^K\n",
    "        \n",
    "        # Test 5: Check that reward is a float\n",
    "        @test isa(reward, Float64) == true\n",
    "        \n",
    "        # Test 6: Test budget constraint penalty application\n",
    "        # High-cost scenario should produce different reward than low-cost\n",
    "        r1 = experiment(contextmodel, 1, 2^K)[2]  # all drugs (expensive)\n",
    "        r2 = experiment(contextmodel, 1, 1)[2]    # one drug (cheaper)\n",
    "        @test isa(r1, Float64) == true && isa(r2, Float64) == true\n",
    "        \n",
    "        # Test 7: Test state transition logic\n",
    "        # Next state should be valid regardless of transition strategy (wrap, random, etc.)\n",
    "        s_last, _ = experiment(contextmodel, m^K, 1)\n",
    "        @test s_last >= 1 && s_last <= m^K  # Should return a valid state\n",
    "    end\n",
    "\n",
    "    @testset \"Context Model Tests\" begin\n",
    "        # Test 8: Check if contextmodel is defined\n",
    "        @test isdefined(Main, :contextmodel) == true\n",
    "        \n",
    "        # Test 9: Check contextmodel type\n",
    "        @test isa(contextmodel, MyExperimentalDrugCocktailContext) == true\n",
    "        \n",
    "        # Test 10: Check required fields in contextmodel\n",
    "        @test hasfield(typeof(contextmodel), :K) == true\n",
    "        @test hasfield(typeof(contextmodel), :m) == true\n",
    "        @test hasfield(typeof(contextmodel), :Î³) == true\n",
    "        @test hasfield(typeof(contextmodel), :B) == true\n",
    "        @test hasfield(typeof(contextmodel), :cost) == true\n",
    "        @test hasfield(typeof(contextmodel), :levels) == true\n",
    "        @test hasfield(typeof(contextmodel), :W) == true\n",
    "        @test hasfield(typeof(contextmodel), :S) == true\n",
    "        @test hasfield(typeof(contextmodel), :bounds) == true\n",
    "        \n",
    "        # Test 11: Check K is positive and reasonable\n",
    "        @test contextmodel.K > 0\n",
    "        @test contextmodel.K == K\n",
    "        \n",
    "        # Test 12: Check m is positive and reasonable\n",
    "        @test contextmodel.m > 1\n",
    "        @test contextmodel.m == m\n",
    "        \n",
    "        # Test 13: Check Î³ array has correct length\n",
    "        @test length(contextmodel.Î³) == contextmodel.K\n",
    "        \n",
    "        # Test 14: Check cost dictionary has K entries\n",
    "        @test length(contextmodel.cost) == contextmodel.K\n",
    "        \n",
    "        # Test 15: Check levels dictionary has K entries\n",
    "        @test length(contextmodel.levels) == contextmodel.K\n",
    "        \n",
    "        # Test 16: Check each drug has high, nominal, low levels\n",
    "        for i in 1:contextmodel.K\n",
    "            @test haskey(contextmodel.levels, i) == true\n",
    "            @test haskey(contextmodel.levels[i], :high) == true\n",
    "            @test haskey(contextmodel.levels[i], :nominal) == true\n",
    "            @test haskey(contextmodel.levels[i], :low) == true\n",
    "            # Verify ordering: low < nominal < high\n",
    "            @test contextmodel.levels[i].low < contextmodel.levels[i].nominal < contextmodel.levels[i].high\n",
    "            # Check all values are positive\n",
    "            @test contextmodel.levels[i].low > 0.0\n",
    "            @test contextmodel.levels[i].nominal > 0.0\n",
    "            @test contextmodel.levels[i].high > 0.0\n",
    "        end\n",
    "        \n",
    "        # Test 17: Check bounds array dimensions\n",
    "        @test size(contextmodel.bounds) == (contextmodel.K, 2)\n",
    "        \n",
    "        # Test 18: Check budget value is defined and positive\n",
    "        @test contextmodel.B > 0.0\n",
    "        \n",
    "        # Test 19: Check patient weight is defined and positive\n",
    "        @test contextmodel.W > 0.0\n",
    "        \n",
    "        # Test 20: Check safety constraint is defined and positive\n",
    "        @test contextmodel.S > 0\n",
    "    end\n",
    "\n",
    "    @testset \"Learning Agent Tests\" begin\n",
    "        # Test 21: Check if mylearningagent is defined\n",
    "        @test isdefined(Main, :mylearningagent) == true\n",
    "        \n",
    "        # Test 22: Check agent type\n",
    "        @test isa(mylearningagent, MyQLearningAgentModel) == true\n",
    "        \n",
    "        # Test 23: Check required fields\n",
    "        @test hasfield(typeof(mylearningagent), :Î³) == true\n",
    "        @test hasfield(typeof(mylearningagent), :Î±) == true\n",
    "        @test hasfield(typeof(mylearningagent), :Q) == true\n",
    "        @test hasfield(typeof(mylearningagent), :states) == true\n",
    "        @test hasfield(typeof(mylearningagent), :actions) == true\n",
    "        \n",
    "        # Test 24: Check discount factor Î³ is valid (0 < Î³ < 1)\n",
    "        @test mylearningagent.Î³ > 0.0 && mylearningagent.Î³ < 1.0\n",
    "        \n",
    "        # Test 25: Check learning rate Î± is valid (0 < Î± â‰¤ 1)\n",
    "        @test mylearningagent.Î± > 0.0 && mylearningagent.Î± â‰¤ 1.0\n",
    "        \n",
    "        # Test 26: Check Q-table dimensions\n",
    "        @test size(mylearningagent.Q) == (length(ğ’®), length(ğ’œ))\n",
    "        \n",
    "        # Test 27: Check Q-table is properly initialized (should be numeric)\n",
    "        @test isa(mylearningagent.Q, Array{Float64}) == true\n",
    "        @test all(isfinite.(mylearningagent.Q)) == true\n",
    "        \n",
    "        # Test 28: Check states array\n",
    "        @test length(mylearningagent.states) == length(ğ’®)\n",
    "        @test mylearningagent.states == ğ’®\n",
    "        \n",
    "        # Test 29: Check actions array\n",
    "        @test length(mylearningagent.actions) == length(ğ’œ)\n",
    "        @test mylearningagent.actions == ğ’œ\n",
    "    end\n",
    "\n",
    "    @testset \"Q-Learning Results Tests\" begin\n",
    "        # Test 30: Check if result is defined\n",
    "        @test isdefined(Main, :result) == true\n",
    "        \n",
    "        # Test 31: Check result has Q field\n",
    "        @test hasfield(typeof(result), :Q) == true\n",
    "        \n",
    "        # Test 32: Check Q-table dimensions match problem size\n",
    "        @test size(result.Q) == (length(ğ’®), length(ğ’œ))\n",
    "        \n",
    "        # Test 33: Q-table should have non-zero values after learning\n",
    "        @test any(result.Q .!= 0.0) == true\n",
    "        \n",
    "        # Test 34: Check Q-values are finite (no NaN or Inf)\n",
    "        @test all(isfinite.(result.Q)) == true\n",
    "        \n",
    "        # Test 35: Check that Q-values span a reasonable range\n",
    "        max_q = maximum(result.Q)\n",
    "        min_q = minimum(result.Q)\n",
    "        @test max_q > min_q  # Should have learned different values\n",
    "        \n",
    "        # Test 36: Check that Q-table is numeric\n",
    "        @test isa(result.Q, Array{Float64}) == true\n",
    "    end\n",
    "\n",
    "    @testset \"Policy Extraction Tests\" begin\n",
    "        # Test 37: Check if policy function Ï€ is defined\n",
    "        @test isdefined(Main, :Ï€) == true\n",
    "        \n",
    "        # Test 38: Check if mypolicy function exists\n",
    "        @test isdefined(Main, :mypolicy) == true\n",
    "        \n",
    "        # Test 39: Policy should return valid action for state 1\n",
    "        action_1 = Ï€(1)\n",
    "        @test action_1 >= 1 && action_1 <= 2^K\n",
    "        \n",
    "        # Test 40: Policy should return integers\n",
    "        test_state_idx = min(5, m^K)\n",
    "        @test isa(Ï€(test_state_idx), Integer) == true\n",
    "        \n",
    "        # Test 41: Test policy for multiple states\n",
    "        num_test_states = min(6, m^K)  # Test up to 6 states if available\n",
    "        test_states = collect(1:num_test_states)\n",
    "        for test_state in test_states\n",
    "            action = Ï€(test_state)\n",
    "            @test action >= 1 && action <= 2^K\n",
    "        end\n",
    "        \n",
    "        # Test 42: Policy should be deterministic (same state â†’ same action)\n",
    "        test_state = min(7, m^K)\n",
    "        action_first = Ï€(test_state)\n",
    "        action_second = Ï€(test_state)\n",
    "        @test action_first == action_second\n",
    "        \n",
    "        # Test 43: mypolicy should return an array of actions\n",
    "        policy_array = mypolicy(result.Q)\n",
    "        @test isa(policy_array, Array) == true\n",
    "        @test length(policy_array) == m^K\n",
    "        \n",
    "        # Test 44: All policy actions should be valid\n",
    "        @test all(policy_array .>= 1) == true\n",
    "        @test all(policy_array .<= 2^K) == true\n",
    "    end\n",
    "\n",
    "    @testset \"Constants and Setup Tests\" begin\n",
    "        # Test 45: Check K constant is defined and reasonable\n",
    "        @test isdefined(Main, :K) == true\n",
    "        @test K > 0 && K < 10  # Should be a reasonable number of drug types\n",
    "        \n",
    "        # Test 46: Check m constant is defined and reasonable\n",
    "        @test isdefined(Main, :m) == true\n",
    "        @test m > 1 && m < 10  # Should have multiple dosage levels\n",
    "        \n",
    "        # Test 47: Check action space ğ’œ\n",
    "        @test isdefined(Main, :ğ’œ) == true\n",
    "        @test length(ğ’œ) == 2^K\n",
    "        @test ğ’œ[1] == 1 && ğ’œ[end] == 2^K\n",
    "        \n",
    "        # Test 48: Check state space ğ’®\n",
    "        @test isdefined(Main, :ğ’®) == true\n",
    "        @test length(ğ’®) == m^K\n",
    "        @test ğ’®[1] == 1 && ğ’®[end] == m^K\n",
    "        \n",
    "        # Test 49: Check W (patient weight) is defined and reasonable\n",
    "        @test isdefined(Main, :W) == true\n",
    "        @test W > 0.0 && W < 500.0  # Reasonable weight range in kg\n",
    "        \n",
    "        # Test 50: Check B (budget) is defined and positive\n",
    "        @test isdefined(Main, :B) == true\n",
    "        @test B > 0.0\n",
    "        \n",
    "        # Test 51: Check S (safety constraint) is defined and positive\n",
    "        @test isdefined(Main, :S) == true\n",
    "        @test S > 0\n",
    "    end\n",
    "\n",
    "    @testset \"Integration Tests\" begin\n",
    "        # Test 52: Run a complete experiment cycle\n",
    "        s_initial = 1\n",
    "        a_test = min(3, 2^K)\n",
    "        s_new, reward = experiment(contextmodel, s_initial, a_test)\n",
    "        @test s_new >= 1 && s_new <= m^K  # Next state should always be valid\n",
    "        @test isa(reward, Float64) == true\n",
    "        \n",
    "        # Test 53: Verify Q-learning improved over initial values\n",
    "        # After learning, Q-values should differ from initial (all zeros)\n",
    "        @test sum(abs.(result.Q)) > 0.0\n",
    "        \n",
    "        # Test 54: Check that learned policy prefers non-trivial actions\n",
    "        # At least some states should have different actions (policy diversity)\n",
    "        policy_diversity = length(unique(mypolicy(result.Q)))\n",
    "        @test policy_diversity >= 1  # Should have at least one action\n",
    "        \n",
    "        # Test 55: Test consistency between Q-values and policy\n",
    "        # The policy should select actions with maximum Q-values\n",
    "        num_random_tests = min(5, m^K)  # Test up to 5 random states\n",
    "        test_states = rand(1:m^K, num_random_tests)\n",
    "        for test_state in test_states\n",
    "            policy_action = Ï€(test_state)\n",
    "            q_values_for_state = result.Q[test_state, :]\n",
    "            max_q_action = argmax(q_values_for_state)\n",
    "            @test policy_action == max_q_action\n",
    "        end\n",
    "    end\n",
    "\n",
    "    @testset \"Completion Confirmation Tests\" begin\n",
    "        # Test 56: Confirm completion of test case 1\n",
    "        @test did_I_do_test_case_1 == true\n",
    "        \n",
    "        # Test 57: Confirm completion of test case 2\n",
    "        @test did_I_do_test_case_2 == true\n",
    "        \n",
    "        # Test 58: Confirm completion of test case 3\n",
    "        @test did_I_do_test_case_3 == true\n",
    "    end\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.1",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
